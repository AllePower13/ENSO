{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione messaggi di logging\n",
    "logging.basicConfig(\n",
    "   level=logging.DEBUG,    # Garantisce che DEBUG passi\n",
    "   format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n",
    "   datefmt=\"%H:%M:%S\",\n",
    "   force=True              # Per Jupyter/ambienti già configurati\n",
    ")\n",
    "\n",
    "# # Imposto il cluster Dask\n",
    "# def main():\n",
    "#    cluster = LocalCluster(\n",
    "#       n_workers=4,           # Numero di processi indipendenti\n",
    "#       threads_per_worker=1,  # Evita oversubscription della CPU\n",
    "#       memory_limit='1GB',    # Limita RAM per worker\n",
    "#       silence_logs=False\n",
    "#    )\n",
    "#    client = Client(cluster)\n",
    "#    print(client.dashboard_link)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#    main()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"OKKKKKK\")\n",
    "#     mp.set_start_method(\"fork\")\n",
    "\n",
    "#     from dask.distributed import Client, LocalCluster\n",
    "\n",
    "#     cluster = LocalCluster(\n",
    "#         n_workers=4,\n",
    "#         threads_per_worker=1,\n",
    "#         memory_limit=\"5GB\"\n",
    "#     )\n",
    "#     client = Client(cluster)\n",
    "#     print(client.dashboard_link)\n",
    "\n",
    "# print(\"fine dei giochi\")\n",
    "# sys.exit()\n",
    "# print(\"cazzo\")\n",
    "\n",
    "# Directories\n",
    "## Home\n",
    "common_dir_home = '/home/montanarini/ELNINO/'\n",
    "dir_output_9 = 'output/Task-9_ROModel/'\n",
    "\n",
    "## Datasets\n",
    "common_dir_ds = '/nas/'\n",
    "### pi-control\n",
    "dir_pi = 'archive_CMIP6/CMIP6/model-output/EC-Earth-Consortium/EC-Earth3/piControl/'\n",
    "dir_pi_tos = 'ocean/Omon/r1i1p1f1_r25/tos/*.nc'\n",
    "### Bottini\n",
    "dir_BOTTINI = 'BOTTINO/irods_data/stabilization-{}/r1i1p1f1/'\n",
    "dir_bxxx = '{}/*.nc'\n",
    "\n",
    "# Constants\n",
    "secondi_mese = 60*60*24*30.43803025425 # Durata media mese: 365.256363051/12\n",
    "\n",
    "# Keywords: datasets, variables and path completion\n",
    "## Datsets\n",
    "dataset_names = ['pi-control', 'b990','b025', 'b050', 'b065', 'b080', 'b100']\n",
    "dataset_names_iteration = len(dataset_names)\n",
    "## Completamento dei path dei bottini\n",
    "bottini_paths = ['hist-1990', 'ssp585-2025', 'ssp585-2050', 'ssp585-2065', 'ssp585-2080', 'ssp585-2100']\n",
    "## Definisco le liste di variaibli (Oceaniche)\n",
    "var_names = ['tos', 'thetao']\n",
    "var_names_iteration = len(var_names)\n",
    "\n",
    "# Parametri\n",
    "# valori da usare (i) confronto per violin-plot: 500, 360, 500; (ii) confronto per scatter-plot: [[1000, 360, 1000], [100, 360, 100]]\n",
    "time_window = 500 # ultimi n anni del Dataset (non per hist e obs), default: 500\n",
    "time_rm = 360 # Mesi in cui si applica la RM, default: 360\n",
    "print(\"\\nThe following set of parameters was selected:\",\n",
    "   \"\\n- Last n years of the bottino runs: \", time_window,\n",
    "   \"\\n- Running Mean time, in months (for the historical detrending): \", time_rm)\n",
    "\n",
    "# Flag per eseguire le cose una volta sola\n",
    "flag_continental = True\n",
    "\n",
    "# Dizionario dei path per i Datasets per ogni variabile\n",
    "directories = {}\n",
    "for i, datasetN in enumerate(dataset_names):\n",
    "   directories[datasetN] = {}\n",
    "   for var in var_names:\n",
    "      # Aggiunta dei path non bottino\n",
    "      if datasetN in ['pi-control']:\n",
    "         directories[datasetN][var] = [common_dir_ds + dir_pi + 'ocean/Omon/r1i1p1f1_r25/' + var + '/*.nc']\n",
    "      # Aggiunta dei path bottino\n",
    "      elif datasetN in ['b990','b025', 'b050', 'b065', 'b080', 'b100']:\n",
    "         directories[datasetN][var] = [(common_dir_ds + dir_BOTTINI).format(bottini_paths[i-1]) + 'Omon_r25/' + dir_bxxx.format(var_names[var_names.index(var)])]\n",
    "\n",
    "# Liste per salvare i tempi impiegati per ogni dataset\n",
    "time_begin_var = [None] * var_names_iteration\n",
    "time_end_var = [None] * var_names_iteration\n",
    "time_var = [None] * var_names_iteration\n",
    "time_begin_ds = [None] * dataset_names_iteration\n",
    "time_end_ds = [None] * dataset_names_iteration\n",
    "\n",
    "# Iterazione sulle Variabili\n",
    "for i, var in enumerate(var_names):\n",
    "   \n",
    "   time_begin_var[i] = datetime.now()\n",
    "\n",
    "   # Seleziona solo la variaibile n ed escludi le altre\n",
    "   # if var not in ['thetao']:\n",
    "   #    time.sleep(1)\n",
    "   #    time_end_var[i] = datetime.now()\n",
    "   #    time_var[i] = (time_end_var[i]-time_begin_var[i])\n",
    "   #    continue\n",
    "\n",
    "   print(f\"\\n# Inizia l'elaborazione della variabile: {var}\")\n",
    "\n",
    "   # Iterazione sui Dataset\n",
    "   for j, datasetN in enumerate(dataset_names):\n",
    "      time_begin_ds[j] = datetime.now()\n",
    "      \n",
    "      # Display informazioni sulla directory \n",
    "      print(f\"\\n## Inizia l'elaborazione di: Dataset_{datasetN}\\nCorresponding path: \", directories[datasetN][var][0])\n",
    "      \n",
    "\n",
    "      print(\"\\nStart of Dataset opening/upload\")\n",
    "\n",
    "      ###################################################################################################################################\n",
    "      #################################### (a) Discrimination based on the Dataset of origin ############################################\n",
    "\n",
    "      if datasetN in ['b990','b025', 'b050', 'b065', 'b080', 'b100']:\n",
    "         # Apertura dataset\n",
    "         Dataset = xr.open_mfdataset(\n",
    "            directories[datasetN][var][0],\n",
    "            use_cftime=True,\n",
    "            parallel=True,\n",
    "            chunks={'time': 100, 'lev': -1, 'lat': 10, 'lon': 50} # NB: le dimensiioni non presenti (lev in tos ad esempio) vengono ignorate\n",
    "         )\n",
    "         \n",
    "      elif datasetN in ['pi-control']:\n",
    "         # Apertura dataset\n",
    "         Dataset = xr.open_mfdataset(\n",
    "            directories[datasetN][var][0],\n",
    "            decode_times=True,\n",
    "            use_cftime=True,\n",
    "            parallel=True,\n",
    "            chunks={'time': 100, 'lev': -1, 'lat': 10, 'lon': 50}\n",
    "         )\n",
    "                         \n",
    "      # Riordinamento coordinate per evitare frammentazione dei chunks\n",
    "      #Dataset = apf.sort_dataset_coords(Dataset)#.rechunk({'time': 10, 'lev': -1, 'lat': 10, 'lon': 50})\n",
    "      \n",
    "      # Discriminazione del range temporale:     \n",
    "      max_year = Dataset['time'].max()\n",
    "      min_year = Dataset['time'].min()\n",
    "      ## Estremi per i Bottini\n",
    "      sup_year_bott = cftime.DatetimeProlepticGregorian(int(max_year.dt.year), 1, 1)\n",
    "      inf_year_bott = cftime.DatetimeProlepticGregorian(int(max_year.dt.year-time_window), 1, 1)\n",
    "      date_format = '%Y-%m-%d'\n",
    "      ## Limitazione del range temporale\n",
    "      Dataset = Dataset.sel(time=slice(inf_year_bott, sup_year_bott))\n",
    "\n",
    "      print(\"\\nStart of time and space variable selection\")\n",
    "      time_begin_var[j] = datetime.now()\n",
    "\n",
    "      if var in ['tos']:\n",
    "         # Passaggio al DataArray\n",
    "         campo = Dataset['tos'].sel(lat=slice(-5, 5), lon=slice(210, 270)) # Nino3 zone for SSTA: eastern equatorial Pacific SSTA (T_E)\n",
    "\n",
    "         # Detrending del campo e calcolo dell'anomalia\n",
    "         campo_detrend, anomaly_detrend, trend_hist, seasonal_cycle = apf.detrending(campo, time_rm)\n",
    "         \n",
    "         # Maschera continentale\n",
    "         if flag_continental == True:\n",
    "            continental_mask = np.isnan(campo)[0,...]\n",
    "            flag_continental = False\n",
    "         \n",
    "         # Calcolo la derivata dell'anomalia\n",
    "         anomaly_detrend = anomaly_detrend.chunk({'time': 100, 'lat': 10, 'lon': 50})  # Rechunking del campo dopo la funzione di detrending(). {'time': -1} se vuoi un unico blocco\n",
    "         Danom_Dt = anomaly_detrend.differentiate('time') * secondi_mese\n",
    "\n",
    "         # Condensazione ad array 1D, discriminazione sul tipo di variaibile: zonale (time, lat, lon) o verticale (time, lat, lon, lev)\n",
    "         #campo_detrend = apf.global_mean(campo_detrend)\n",
    "         anomaly_detrend = apf.global_mean(anomaly_detrend)\n",
    "         Danom_Dt = apf.global_mean(Danom_Dt)\n",
    "\n",
    "         # Salvataggio dei campi\n",
    "         print(\"\\nStart saving the evaluated variables. Dataset: \", datasetN, \", Variable :\", var)\n",
    "         ds = xr.Dataset(\n",
    "            data_vars=dict(\n",
    "               anomaly = anomaly_detrend,\n",
    "               Danom_Dt = Danom_Dt,\n",
    "            )\n",
    "         ).to_netcdf(common_dir_home + dir_output_9 + f'Task-9_{datasetN}-{var}.nc', compute=True)\n",
    "\n",
    "      elif var in ['thetao']:\n",
    "         # Passaggio al DataArray\n",
    "         campo = Dataset['thetao'].sel(lev=slice(0,300), lat=slice(-5, 5), lon=slice(140, 205)) # Warm Pool region for Thermocline Depth Anomaly (hw) (Original: [120, 205])), limitation to the Mixing Layer.\n",
    "         \n",
    "         # Controllo dei chunks\n",
    "         ## Informazioni sui chunk\n",
    "         chunks = campo.data.chunks  # tuple di tuple: una per dimensione\n",
    "         dtype_size = campo.data.dtype.itemsize  # byte per elemento\n",
    "         ## Calcola il numero di elementi per chunk (in genere prendiamo il primo per stimare)\n",
    "         chunk_shape = tuple(dim[0] for dim in chunks)\n",
    "         n_elements = np.prod(chunk_shape)\n",
    "         bytes_per_chunk = n_elements * dtype_size\n",
    "         print(f\"Chunk shape: {chunk_shape}\")\n",
    "         print(f\"Bytes per chunk: {bytes_per_chunk / 1e6:.2f} MB\")\n",
    "\n",
    "         # Detrending del campo e calcolo dell'anomalia\n",
    "         campo_detrend, anomaly_detrend, trend_hist, seasonal_cycle = apf.detrending(campo, time_rm)\n",
    "         \n",
    "         # Preparazione al calcolo della termoclina\n",
    "         ## Coordinate\n",
    "         lev = campo_detrend['lev']\n",
    "         lon = campo_detrend['lon']\n",
    "         ## Definizione dimensioni estese per interpolazioni\n",
    "         lev_ext = xr.DataArray(np.linspace(lev.min().values, lev.max().values, 500), dims=['lev'])\n",
    "         lon_ext = np.linspace(lon.min(), lon.max(), 500)\n",
    "         lon_ext_coord = xr.DataArray(lon_ext, dims=['lon_ext']) # Crea un DataArray per allineare le coordinat\n",
    "         \n",
    "         ## Media latitudinale\n",
    "         campo_tofunc = apf.meridional_mean(campo_detrend)\n",
    "\n",
    "         ## Applico l'interpolazione alla coordinata di profondità (z) -'lev'- per ogni punto di longitudine e ricalcolo il valore del campo nei nuovi punti\n",
    "         # Ci sono più modi per interpolare:\n",
    "         #  - Modo 1, interpolazione semplice: DataArrray.interp()\n",
    "         #  - campo_tofunc_interp = campo_tofunc.interp(coords={'lev': lev_ext}, method=\"linear\")  # Dask-friendly e vettorizzato.\n",
    "         #  - Modo 2: interpolazione spline, più precisa e accurata. Si può ottenere solo con funzioni non Dask friendly di Scipy.interpolate: interp1d(), splrep() + splev(); CubicSpline()\n",
    "         #  che sono da applicare via 'apply_ufunc()' o 'map_blocks()' più flessibile specialmente per datasets chunked. Sono riuscito a eseguire una spline solo con interp1d(), con le altre funzioni ho fallito.\n",
    "         campo_tofunc_interp = apf.spline_interpolation_along_dim(\n",
    "            campo_tofunc,\n",
    "            lev_ext,\n",
    "            \"lev\",\n",
    "            \"cubic\"\n",
    "         )#.transpose('time', 'lon', 'lev').chunk({'time': 100, 'lon': 50}) # Riordino le dimensioni in modo che ci siano per prima le non core ('time', 'lat') e poi le core ('lon', 'lev') e forzo la coerenza con un chunking\n",
    "         \n",
    "         # Calcolod della Termoclina \n",
    "         interpol = False # Flag per abilitare o meno l'interpolazione\n",
    "         termocline, depth_max_grad, temp_max_grad = apf.thermocline(campo_tofunc_interp, interpol) # La termoclina restituitita ha valori positivi sull'asse 'lev' , quindi sono da invertire moltiplicando *(-1)\n",
    "         \n",
    "         ## Anomalia\n",
    "         termocline_anom = termocline - termocline.mean('time')\n",
    "         \n",
    "         ## Calcolo della derivata della termoclina\n",
    "         #termocline_anom = termocline_anom.chunk({'time': -1})  # Rechunking del campo dopo la funzione di detrending()\n",
    "         Dtermocline_anom_Dt = termocline_anom.differentiate('time')\n",
    "         \n",
    "         ## Riduzione dimensionale ad array 1D\n",
    "         termocline_anom = termocline_anom.mean('lon')*(-1)\n",
    "         Dtermocline_anom_Dt = Dtermocline_anom_Dt.mean('lon')*secondi_mese*(-1)\n",
    "\n",
    "         # Salvataggio dei campi\n",
    "         print(\"\\nStart saving the evaluated variables. Dataset: \", datasetN, \", Variable :\", var)\n",
    "            \n",
    "         ds = xr.Dataset(\n",
    "            data_vars=dict(\n",
    "               termocline_anom = termocline_anom,\n",
    "               Dtermocline_anom_Dt = Dtermocline_anom_Dt,\n",
    "            )\n",
    "         ).to_netcdf(common_dir_home + dir_output_9 + f'Task-9_{datasetN}-{var}.nc', compute=True)\n",
    "      \n",
    "      ### Uscita dal loop sui Dataset ###\n",
    "   \n",
    "      # Dataset time recap\n",
    "      time_end_ds[j] = datetime.now()\n",
    "      print(\"\\nEnd dataset elaboration\")\n",
    "      print(\"Dataset {} took {}\".format(datasetN, time_end_ds[j]-time_begin_ds[j]))\n",
    "\n",
    "   ### Uscita dal loop sulle variabili ###\n",
    "   \n",
    "   # Variable time recap\n",
    "   time_end_var[i] = datetime.now()\n",
    "   time_var[i] = (time_end_var[i]-time_begin_var[i])\n",
    "   print(\"\\nEnd of variable evaluation\")\n",
    "   print(\"Variable {} took {}\".format(var_names[i], time_var[i]))\n",
    "\n",
    "# Time recap\n",
    "print(\"\\n## Timing recap\")\n",
    "for i, var in enumerate(var_names):\n",
    "   print(\"\\nOverall evaluation of \", var, \" for all the Datasets took {}\".format(time_var[i]))\n",
    "\n",
    "print(\"\\nOverall evaluation of the script was completed in {}\".format(np.sum(time_var)))\n",
    "print(\"\\nEnd of the script\")\n",
    "\n",
    "# Chiusura client e cluster Dask\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9676b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task 9 Recharge Oscillator Model\n",
    "\n",
    "- Script di calcolo delle _anomalie_ dei campi e le rispettive _derivate_ di 'tos' e 'thetao' per i dataset Bottino + Pi-control,\n",
    "per poterle utilizzare come serie temporali nell'equazione del modello di Jin del Recharge-Oscillator attraverso la semplificazione del modello lineare di regressione.\n",
    "Delle variaibili citate, dopo averle isolate nele regioni di definizione presenti nell'articolo di Jin 2021, viene eseguito il detrending, e la condensazione a serie temporali (in cui l'unica dimensione rimanenete è il tempo).\n",
    "Per 'thetao' viene calcolata la termoclina attraverso 2 funzioni complesse Dask-friendly anche via chunks, la prima interpola sulla dimensione 'lev', la seconda calcola la termoclina.\n",
    "\n",
    "Nota: Ho cercato di eseguire la miglior ottimizzazione possibile con Dask, attraverso anche la definizione dei 'chunks'.\n",
    "Tuttavia, visto che lavoro solo su una piccola fetta del mondo (e non su tutto il mondo) i chunk non sono ottimizzati, sono troppo piccoli e mi viene dato un messaggio di WARNING:\n",
    "\"/home/montanarini/miniforge3/envs/ctl4b/lib/python3.9/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 30 times more chunks\"\n",
    "Ad ogni modo la pipeline di Dask costituisce la base di partenza per altri scripts.\n",
    "\"\"\"\n",
    "# import libraries\n",
    "## System libraries\n",
    "import sys\n",
    "import logging\n",
    "## General data analisys libraries \n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from dask.distributed import Client, LocalCluster\n",
    "## Time library\n",
    "import cftime\n",
    "from datetime import datetime\n",
    "# Personal library\n",
    "import AllePowerFunctions as apf\n",
    "\n",
    "print(__name__, \" has been launched on date:\", datetime.now(), \"\\n\")\n",
    "\n",
    "# Configurazione messaggi di logging\n",
    "logging.basicConfig(\n",
    "   level=logging.DEBUG,    # Garantisce che DEBUG passi\n",
    "   format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n",
    "   datefmt=\"%H:%M:%S\",\n",
    "   force=True              # Per Jupyter/ambienti già configurati\n",
    ")\n",
    "\n",
    "# Imposto il cluster Dask\n",
    "def main():\n",
    "   cluster = LocalCluster(\n",
    "      n_workers=4,           # Numero di processi indipendenti\n",
    "      threads_per_worker=1,  # Evita oversubscription della CPU\n",
    "      memory_limit='5GB'    # Limita RAM per worker\n",
    "   )\n",
    "   client = Client(cluster)\n",
    "   print(client.dashboard_link)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()\n",
    "\n",
    "# Directories\n",
    "## Home\n",
    "common_dir_home = '/home/montanarini/ELNINO/'\n",
    "dir_output_9 = 'output/Task-9_ROModel/'\n",
    "\n",
    "## Datasets\n",
    "common_dir_ds = '/nas/'\n",
    "### pi-control\n",
    "dir_pi = 'archive_CMIP6/CMIP6/model-output/EC-Earth-Consortium/EC-Earth3/piControl/'\n",
    "dir_pi_tos = 'ocean/Omon/r1i1p1f1_r25/tos/*.nc'\n",
    "### Bottini\n",
    "dir_BOTTINI = 'BOTTINO/irods_data/stabilization-{}/r1i1p1f1/'\n",
    "dir_bxxx = '{}/*.nc'\n",
    "\n",
    "# Constants\n",
    "secondi_mese = 60*60*24*30.43803025425 # Durata media mese: 365.256363051/12\n",
    "\n",
    "# Keywords: datasets, variables and path completion\n",
    "## Datsets\n",
    "dataset_names = ['pi-control', 'b990','b025', 'b050', 'b065', 'b080', 'b100']\n",
    "dataset_names_iteration = len(dataset_names)\n",
    "## Completamento dei path dei bottini\n",
    "bottini_paths = ['hist-1990','ssp585-2025', 'ssp585-2050', 'ssp585-2065', 'ssp585-2080', 'ssp585-2100']\n",
    "## Definisco le liste di variaibli (Oceaniche)\n",
    "var_names = ['tos', 'thetao']\n",
    "var_names_iteration = len(var_names)\n",
    "\n",
    "# Parametri\n",
    "# valori da usare (i) confronto per violin-plot: 500, 360, 500; (ii) confronto per scatter-plot: [[1000, 360, 1000], [100, 360, 100]]\n",
    "time_window = 500 # ultimi n anni del Dataset (non per hist e obs), default: 500\n",
    "time_rm = 360 # Mesi in cui si applica la RM, default: 360\n",
    "print(\"\\nThe following set of parameters was selected:\",\n",
    "   \"\\n- Last n years of the bottino runs: \", time_window,\n",
    "   \"\\n- Running Mean time, in months (for the historical detrending): \", time_rm)\n",
    "\n",
    "# Flag per eseguire le cose una volta sola\n",
    "flag_continental = True\n",
    "\n",
    "# Dizionario dei path per i Datasets per ogni variabile\n",
    "directories = {}\n",
    "for i, datasetN in enumerate(dataset_names):\n",
    "   directories[datasetN] = {}\n",
    "   for var in var_names:\n",
    "      # Aggiunta dei path non bottino\n",
    "      if datasetN in ['pi-control']:\n",
    "         directories[datasetN][var] = [common_dir_ds + dir_pi + 'ocean/Omon/r1i1p1f1_r25/' + var + '/*.nc']\n",
    "      # Aggiunta dei path bottino\n",
    "      elif datasetN in ['b990','b025', 'b050', 'b065', 'b080', 'b100']:\n",
    "         directories[datasetN][var] = [(common_dir_ds + dir_BOTTINI).format(bottini_paths[i-1]) + 'Omon_r25/' + dir_bxxx.format(var_names[var_names.index(var)])]\n",
    "\n",
    "# Liste per salvare i tempi impiegati per ogni dataset\n",
    "time_begin_var = [None] * var_names_iteration\n",
    "time_end_var = [None] * var_names_iteration\n",
    "time_var = [None] * var_names_iteration\n",
    "time_begin_ds = [None] * dataset_names_iteration\n",
    "time_end_ds = [None] * dataset_names_iteration\n",
    "      \n",
    "# Iterazione sulle Variabili\n",
    "for i, var in enumerate(var_names):\n",
    "   \n",
    "   time_begin_var[i] = datetime.now()\n",
    "\n",
    "   # Seleziona solo la variaibile n ed escludi le altre\n",
    "   # if var not in ['thetao']:\n",
    "   #    time.sleep(1)\n",
    "   #    time_end_var[i] = datetime.now()\n",
    "   #    time_var[i] = (time_end_var[i]-time_begin_var[i])\n",
    "   #    continue\n",
    "\n",
    "   print(f\"\\n# Inizia l'elaborazione della variabile: {var}\")\n",
    "\n",
    "   # Iterazione sui Dataset\n",
    "   for j, datasetN in enumerate(dataset_names):\n",
    "      time_begin_ds[j] = datetime.now()\n",
    "      \n",
    "      # Display informazioni sulla directory \n",
    "      print(f\"\\n## Inizia l'elaborazione di: Dataset_{datasetN}\\nCorresponding path: \", directories[datasetN][var][0])\n",
    "      \n",
    "\n",
    "      print(\"\\nStart of Dataset opening/upload\")\n",
    "\n",
    "      ###################################################################################################################################\n",
    "      #################################### (a) Discrimination based on the Dataset of origin ############################################\n",
    "\n",
    "      if datasetN in ['b990','b025', 'b050', 'b065', 'b080', 'b100']:\n",
    "         # Apertura dataset\n",
    "         Dataset = xr.open_mfdataset(\n",
    "            directories[datasetN][var][0],\n",
    "            use_cftime=True,\n",
    "            parallel=True,\n",
    "            chunks={'time': 100, 'lev': -1, 'lat': 10, 'lon': 50} # NB: le dimensiioni non presenti (lev in tos ad esempio) vengono ignorate\n",
    "         )\n",
    "         \n",
    "      elif datasetN in ['pi-control']:\n",
    "         # Apertura dataset\n",
    "         Dataset = xr.open_mfdataset(\n",
    "            directories[datasetN][var][0],\n",
    "            decode_times=True,\n",
    "            use_cftime=True,\n",
    "            parallel=True,\n",
    "            chunks={'time': 100, 'lev': -1, 'lat': 10, 'lon': 50}\n",
    "         )\n",
    "                         \n",
    "      # Riordinamento coordinate per evitare frammentazione dei chunks\n",
    "      #Dataset = apf.sort_dataset_coords(Dataset)#.rechunk({'time': 10, 'lev': -1, 'lat': 10, 'lon': 50})\n",
    "      \n",
    "      # Discriminazione del range temporale:     \n",
    "      max_year = Dataset['time'].max()\n",
    "      min_year = Dataset['time'].min()\n",
    "      ## Estremi per i Bottini\n",
    "      sup_year_bott = cftime.DatetimeProlepticGregorian(int(max_year.dt.year), 1, 1)\n",
    "      inf_year_bott = cftime.DatetimeProlepticGregorian(int(max_year.dt.year-time_window), 1, 1)\n",
    "      date_format = '%Y-%m-%d'\n",
    "      ## Limitazione del range temporale\n",
    "      Dataset = Dataset.sel(time=slice(inf_year_bott, sup_year_bott))\n",
    "\n",
    "      print(\"\\nStart of time and space variable selection\")\n",
    "      time_begin_var[j] = datetime.now()\n",
    "\n",
    "      if var in ['tos']:\n",
    "         # Passaggio al DataArray\n",
    "         campo = Dataset['tos'].sel(lat=slice(-5, 5), lon=slice(210, 270)) # Nino3 zone for SSTA: eastern equatorial Pacific SSTA (T_E)\n",
    "\n",
    "         # Detrending del campo e calcolo dell'anomalia\n",
    "         campo_detrend, anomaly_detrend, trend_hist, seasonal_cycle = apf.detrending(campo, time_rm)\n",
    "         \n",
    "         # Maschera continentale\n",
    "         if flag_continental == True:\n",
    "            continental_mask = np.isnan(campo)[0,...]\n",
    "            flag_continental = False\n",
    "         \n",
    "         # Calcolo la derivata dell'anomalia\n",
    "         anomaly_detrend = anomaly_detrend.chunk({'time': 100, 'lat': 10, 'lon': 50})  # Rechunking del campo dopo la funzione di detrending(). {'time': -1} se vuoi un unico blocco\n",
    "         Danom_Dt = anomaly_detrend.differentiate('time') * secondi_mese\n",
    "\n",
    "         # Condensazione ad array 1D, discriminazione sul tipo di variaibile: zonale (time, lat, lon) o verticale (time, lat, lon, lev)\n",
    "         #campo_detrend = apf.global_mean(campo_detrend)\n",
    "         anomaly_detrend = apf.global_mean(anomaly_detrend)\n",
    "         Danom_Dt = apf.global_mean(Danom_Dt)\n",
    "\n",
    "         # Salvataggio dei campi\n",
    "         print(\"\\nStart saving the evaluated variables. Dataset: \", datasetN, \", Variable :\", var)\n",
    "         ds = xr.Dataset(\n",
    "            data_vars=dict(\n",
    "               anomaly = anomaly_detrend,\n",
    "               Danom_Dt = Danom_Dt,\n",
    "            )\n",
    "         ).to_netcdf(common_dir_home + dir_output_9 + f'Task-9_{datasetN}-{var}.nc', compute=True)\n",
    "\n",
    "      elif var in ['thetao']:\n",
    "         # Passaggio al DataArray\n",
    "         campo = Dataset['thetao'].sel(lev=slice(0,300), lat=slice(-5, 5), lon=slice(140, 205)) # Warm Pool region for Thermocline Depth Anomaly (hw) (Original: [120, 205])), limitation to the Mixing Layer.\n",
    "         \n",
    "         # Controllo dei chunks\n",
    "         ## Informazioni sui chunk\n",
    "         chunks = campo.data.chunks  # tuple di tuple: una per dimensione\n",
    "         dtype_size = campo.data.dtype.itemsize  # byte per elemento\n",
    "         ## Calcola il numero di elementi per chunk (in genere prendiamo il primo per stimare)\n",
    "         chunk_shape = tuple(dim[0] for dim in chunks)\n",
    "         n_elements = np.prod(chunk_shape)\n",
    "         bytes_per_chunk = n_elements * dtype_size\n",
    "         print(f\"Chunk shape: {chunk_shape}\")\n",
    "         print(f\"Bytes per chunk: {bytes_per_chunk / 1e6:.2f} MB\")\n",
    "\n",
    "         # Detrending del campo e calcolo dell'anomalia\n",
    "         campo_detrend, anomaly_detrend, trend_hist, seasonal_cycle = apf.detrending(campo, time_rm)\n",
    "         \n",
    "         # Preparazione al calcolo della termoclina\n",
    "         ## Coordinate\n",
    "         lev = campo_detrend['lev']\n",
    "         lon = campo_detrend['lon']\n",
    "         ## Definizione dimensioni estese per interpolazioni\n",
    "         lev_ext = xr.DataArray(np.linspace(lev.min().values, lev.max().values, 500), dims=['lev'])\n",
    "         lon_ext = np.linspace(lon.min(), lon.max(), 500)\n",
    "         lon_ext_coord = xr.DataArray(lon_ext, dims=['lon_ext']) # Crea un DataArray per allineare le coordinat\n",
    "         \n",
    "         ## Media latitudinale\n",
    "         campo_tofunc = apf.meridional_mean(campo_detrend)\n",
    "\n",
    "         ## Applico l'interpolazione alla coordinata di profondità (z) -'lev'- per ogni punto di longitudine e ricalcolo il valore del campo nei nuovi punti\n",
    "         # Ci sono più modi per interpolare:\n",
    "         #  - Modo 1, interpolazione semplice: DataArrray.interp()\n",
    "         #  - campo_tofunc_interp = campo_tofunc.interp(coords={'lev': lev_ext}, method=\"linear\")  # Dask-friendly e vettorizzato.\n",
    "         #  - Modo 2: interpolazione spline, più precisa e accurata. Si può ottenere solo con funzioni non Dask friendly di Scipy.interpolate: interp1d(), splrep() + splev(); CubicSpline()\n",
    "         #  che sono da applicare via 'apply_ufunc()' o 'map_blocks()' più flessibile specialmente per datasets chunked. Sono riuscito a eseguire una spline solo con interp1d(), con le altre funzioni ho fallito.\n",
    "         campo_tofunc_interp = apf.spline_interpolation_along_dim(\n",
    "            campo_tofunc,\n",
    "            lev_ext,\n",
    "            \"lev\",\n",
    "            \"cubic\"\n",
    "         )#.transpose('time', 'lon', 'lev').chunk({'time': 100, 'lon': 50}) # Riordino le dimensioni in modo che ci siano per prima le non core ('time', 'lat') e poi le core ('lon', 'lev') e forzo la coerenza con un chunking\n",
    "         \n",
    "         # Calcolod della Termoclina \n",
    "         interpol = False # Flag per abilitare o meno l'interpolazione\n",
    "         termocline, depth_max_grad, temp_max_grad = apf.thermocline(campo_tofunc_interp, interpol) # La termoclina restituitita ha valori positivi sull'asse 'lev' , quindi sono da invertire moltiplicando *(-1)\n",
    "         \n",
    "         ## Anomalia\n",
    "         termocline_anom = termocline - termocline.mean('time')\n",
    "         \n",
    "         ## Calcolo della derivata della termoclina\n",
    "         #termocline_anom = termocline_anom.chunk({'time': -1})  # Rechunking del campo dopo la funzione di detrending()\n",
    "         Dtermocline_anom_Dt = termocline_anom.differentiate('time')\n",
    "         \n",
    "         ## Riduzione dimensionale ad array 1D\n",
    "         termocline_anom = termocline_anom.mean('lon')*(-1)\n",
    "         Dtermocline_anom_Dt = Dtermocline_anom_Dt.mean('lon')*secondi_mese*(-1)\n",
    "\n",
    "         # Salvataggio dei campi\n",
    "         print(\"\\nStart saving the evaluated variables. Dataset: \", datasetN, \", Variable :\", var)\n",
    "            \n",
    "         ds = xr.Dataset(\n",
    "            data_vars=dict(\n",
    "               termocline_anom = termocline_anom,\n",
    "               Dtermocline_anom_Dt = Dtermocline_anom_Dt,\n",
    "            )\n",
    "         ).to_netcdf(common_dir_home + dir_output_9 + f'Task-9_{datasetN}-{var}.nc', compute=True)\n",
    "      \n",
    "      ### Uscita dal loop sui Dataset ###\n",
    "   \n",
    "      # Dataset time recap\n",
    "      time_end_ds[j] = datetime.now()\n",
    "      print(\"\\nEnd dataset elaboration\")\n",
    "      print(\"Dataset {} took {}\".format(datasetN, time_end_ds[j]-time_begin_ds[j]))\n",
    "\n",
    "   ### Uscita dal loop sulle variabili ###\n",
    "   \n",
    "   # Variable time recap\n",
    "   time_end_var[i] = datetime.now()\n",
    "   time_var[i] = (time_end_var[i]-time_begin_var[i])\n",
    "   print(\"\\nEnd of variable evaluation\")\n",
    "   print(\"Variable {} took {}\".format(var_names[i], time_var[i]))\n",
    "\n",
    "# Time recap\n",
    "print(\"\\n## Timing recap\")\n",
    "for i, var in enumerate(var_names):\n",
    "   print(\"\\nOverall evaluation of \", var, \" for all the Datasets took {}\".format(time_var[i]))\n",
    "\n",
    "print(\"\\nOverall evaluation of the script was completed in {}\".format(np.sum(time_var)))\n",
    "print(\"\\nEnd of the script\")\n",
    "\n",
    "# Chiusura client e cluster Dask\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "print(\"\\nEnd of the script\")\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione Termoclina funzionante\n",
    "def thermocline(\n",
    "    campo: xr.DataArray,\n",
    "    lon_ext: np.ndarray,\n",
    "    chunks: Optional[dict] = None,\n",
    "    levels: Optional[np.ndarray] = None,\n",
    "    latitude: Optional[np.ndarray] = None,\n",
    "    longitude: Optional[np.ndarray] = None\n",
    ") -> Tuple[xr.DataArray, xr.DataArray, xr.DataArray]:\n",
    "    \"\"\"\n",
    "    Calcola la termoclina come punto di massimo gradiente verticale.\n",
    "    Versione sia per Xarray, Dask-friendly anche per Dataset chunakti, che per Numpy.ndarray.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    campo :\n",
    "        1° caso, xr.DataArray: campo di temperatura con dimensioni (time, lev, lat, lon) o subset\n",
    "        2° caso, numpy.ndarray: è importante che le dimensioni siano ordinate come mostrato.\n",
    "            Se le dimensioni sono 3 vanno portate a 2, altrimenti è ambiguo. NB in questo caso il Tempo viene ridotto, mediato via,\n",
    "            mentre la versione Xarray parallelizza su 'time'. Adattala nel caso tale dimensione serva.\n",
    "            Questo ramo NON è dask friendly, lo può diventare via 'apply_ufunc' che puoi eseguire dentro la funzione stessa (questa, modificandola) che fuori, cioè nel codice.\n",
    "    lon_ext : np.ndarray\n",
    "        Nuove longitudini su cui interpolare la termoclina\n",
    "    levels : np.ndarray, optional\n",
    "        Livelli di profondità (solo per input numpy)\n",
    "    latitude : np.ndarray, optional  \n",
    "        Latitudini (solo per input numpy)\n",
    "    longitude : np.ndarray, optional\n",
    "        Longitudini (solo per input numpy)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    termoclina_interp : xr.DataArray\n",
    "        Profondità della termoclina interpolata su lon_ext\n",
    "    depth_max_grad : xr.DataArray  \n",
    "        Profondità della termoclina per ogni longitudine originale\n",
    "    temp_max_grad : xr.DataArray\n",
    "        Temperatura alla termoclina per ogni longitudine originale\n",
    "    \"\"\"\n",
    "    if isinstance(campo, xr.DataArray):\n",
    "        # 0. Preparazione\n",
    "        ## 0.1 Verifica Chunks\n",
    "        if isinstance(campo.data, dask_array.Array):\n",
    "            if chunks:\n",
    "                campo = campo.chunk(chunks)\n",
    "                # lev non deve essere chunkata\n",
    "                lev_chunks = chunks[campo.dims.index('lev')] if 'lev' in campo.dims else ()\n",
    "                if len(lev_chunks) > 1 or (lev_chunks and lev_chunks[0] != campo.lev.size):\n",
    "                    raise ValueError(\n",
    "                        \"La dimensione 'lev' deve essere interamente contenuta in un singolo chunk. \"\n",
    "                        \"Usa campo.chunk({'lev': -1}) prima di chiamare questa funzione.\"\n",
    "                    )                \n",
    "            else:\n",
    "                # Chunks di default\n",
    "                campo = campo.chunk({'time': 120, 'lev': -1, 'lon': 27})            \n",
    "        \n",
    "        ## 0.2 Clona il DataArray per non modificare l'originale\n",
    "        campo_work = campo.copy()\n",
    "        \n",
    "        ## 0.3 Media su 'lat'\n",
    "        if 'lat' in campo_work.dims:\n",
    "            try:\n",
    "                campo_work = meridional_mean(campo_work, latitude=None, mask=None, skip_nan=True, skip_inf=True)\n",
    "            except:\n",
    "                # Fallback a media semplice se meridional_mean non disponibile\n",
    "                campo_work = campo_work.mean('lat')\n",
    "                logger.debug(\"Fallback, media latitudinale non pesata\")\n",
    "\n",
    "        ## 0.4 Gradiente\n",
    "        dT_dlev = campo_work.differentiate('lev')\n",
    "        abs_dT_dlev = np.abs(dT_dlev)\n",
    "        \n",
    "        ## 0.5 Massimo del gradiente usando funzione apposita\n",
    "        #il fancy indexing con 'isel' non è gestito da Dask, quindi non posso fare semplicemente come segue,\n",
    "        #devo passare da numpy e di conseguenza apply_ufunc\n",
    "        #idx_max_grad = abs_dT_dlev.argmax(dim='lev')\n",
    "        #depth_max_grad = campo_work['lev'].isel(lev=idx_max_grad)\n",
    "        #temp_max_grad = campo_work.isel(lev=idx_max_grad)\n",
    "        def _find_max_grad(temperature, levels, abs_grad):\n",
    "            \"\"\"\n",
    "            Trova indice, profondità e temperatura del massimo gradiente.\n",
    "            \"\"\"\n",
    "            # Trova indice del massimo gradiente\n",
    "            idx_max = np.nanargmax(abs_grad, axis=0)\n",
    "            \n",
    "            # Estrai profondità corrispondente\n",
    "            depth_max = np.take_along_axis(\n",
    "                levels.reshape(-1, 1), \n",
    "                idx_max.reshape(1, -1), \n",
    "                axis=0\n",
    "            ).squeeze()\n",
    "            \n",
    "            # Estrai temperatura corrispondente\n",
    "            temp_max = np.take_along_axis(\n",
    "                temperature, \n",
    "                idx_max.reshape(1, -1), \n",
    "                axis=0\n",
    "            ).squeeze()\n",
    "            \n",
    "            return depth_max, temp_max, idx_max\n",
    "        \n",
    "        ### 0.5.1 Applica la funzione usando 'apply_ufunc'\n",
    "        depth_max_grad, temp_max_grad, idx_max_grad = xr.apply_ufunc(\n",
    "            _find_max_grad,                                                     # Funzione da applicare\n",
    "            campo_work,                                                         # Input -1° parametro-: Campo a cui applicarla\n",
    "            campo_work['lev'],                                                  # Input -2° parametro-: Vettore delle coordinate dei livelli\n",
    "            abs_dT_dlev,                                                        # Input -3° parametro-: Valore assoluto gradiente\n",
    "            input_core_dims=[['lev', 'lon'], ['lev'], ['lev', 'lon']],          # Dimensioni da trattare come \"core\"\n",
    "            output_core_dims=[['lon'], ['lon'], ['lon']],                       # Dimensioni degli output\n",
    "            vectorize=True,                                                     # Vectorizza su tutte le altre dimensioni (es: time)\n",
    "            dask='parallelized',                                                # Forza la parallelizzazione\n",
    "            output_dtypes=[campo_work['lev'].dtype, campo_work.dtype, np.int64] # Data type of the retourned array\n",
    "        )\n",
    "        # 1. Interpolazione\n",
    "        ## 1.1 funzione interpolante per array 1D su lon_ext\n",
    "        def _spline_cubic_1d(x_old, y_old, x_new):\n",
    "              \"\"\"Funzione interna per interpolazione spline 1D\"\"\"\n",
    "              # Gestione valori NaN\n",
    "              valid_mask = np.isfinite(y_old)\n",
    "              if np.sum(valid_mask) < 2:\n",
    "                 return np.full_like(x_new, np.nan)\n",
    "              \n",
    "              x_valid = x_old[valid_mask]\n",
    "              y_valid = y_old[valid_mask]\n",
    "               \n",
    "              try:\n",
    "                 # Interpolazione spline cubica\n",
    "                 interpolator = interp1d(\n",
    "                    x_valid, y_valid, \n",
    "                    kind='cubic', \n",
    "                    bounds_error=False,\n",
    "                    fill_value=np.nan,\n",
    "                    assume_sorted=True\n",
    "                 )\n",
    "                 return interpolator(x_new)\n",
    "              except (ValueError, TypeError) as e:\n",
    "                 # Fallback a interpolazione lineare\n",
    "                 logger.debug(f\"Spline failed, using linear: {e}\")\n",
    "                 return np.interp(x_new, x_valid, y_valid, left=np.nan, right=np.nan)\n",
    "\n",
    "        ## 1.2 Applica interpolazione usando 'apply_ufunc'\n",
    "        termoclina_interp = xr.apply_ufunc(\n",
    "              _spline_cubic_1d,                                               # Funzione da applicare\n",
    "              campo_work['lon'],                                              # Input: Vettore delle coordinate ascisse originale: x_old\n",
    "              depth_max_grad,                                                 # Input: Vettore delle coordinate ordinate originale: y_old\n",
    "              lon_ext,                                                        # Input: Vettore delle coordinate x di output (per la spline), nuova ascissa: x_new\n",
    "              input_core_dims=[['lon'], ['lon'], ['lon_ext']],                # Dimensioni da trattare come \"core\" (1D)\n",
    "              output_core_dims=[['lon_ext']],                                 # Dimensione degli output\n",
    "              vectorize=True,                                                 # Vectorizza su tutte le altre dimensioni (es: time)\n",
    "              dask='parallelized',                                            # Forza la parallelizzazione\n",
    "              output_dtypes=[depth_max_grad.dtype],                           # Data type of the retourned array\n",
    "              dask_gufunc_kwargs={'output_sizes': {'lon_ext': len(lon_ext)}}\n",
    "        )\n",
    "\n",
    "        ## 1.3 Aggiungi coordinate al risultato\n",
    "        termoclina_interp = termoclina_interp.assign_coords({'lon_ext': lon_ext})\n",
    "        termoclina_interp = termoclina_interp.rename({'lon_ext': 'lon'})\n",
    "        \n",
    "        # 2. Controllo (opzionale - da eseguire solo se non lazy)\n",
    "        if not isinstance(termoclina_interp.data, dask_array.Array):\n",
    "            for output in [termoclina_interp, depth_max_grad, temp_max_grad]:\n",
    "                if np.any(np.isnan(output)):\n",
    "                    logger.warning(\"Risultato contiene NaN\")\n",
    "        \n",
    "        return termoclina_interp, depth_max_grad, temp_max_grad\n",
    "    \n",
    "    elif isinstance(campo, xr.Dataset):\n",
    "        raise ValueError(\"Passa un DataArray invece di un Dataset\")\n",
    "    \n",
    "    elif isinstance(campo, np.ndarray):\n",
    "        # 0. Preparazione\n",
    "        ## 0.1 Verifica se le coordinate sono state fornite\n",
    "        if longitude is None:\n",
    "            raise ValueError(\"Se 'campo' è un numpy.ndarray, l'array delle coordinate 'lon' deve essere fornito.\")\n",
    "        \n",
    "        ## 0.2 Media su tempo e latitudine\n",
    "        if np.ndim(campo) == 4:\n",
    "            # Media sull'asse 0 ('time')\n",
    "            campo = np.average(campo, axis=0) \n",
    "            # Media sull'asse 1 (ora asse 0) ('lat')\n",
    "            campo = meridional_mean(campo, latitude=latitude, mask=None, skip_nan=True, skip_inf=True)\n",
    "        elif (np.ndim(campo) == 3) or (np.ndim(campo)>4):\n",
    "            print(\"Ambiguità con 3 dimensioni, riducile a 2!\")\n",
    "        \n",
    "        # 1. Massimo del gradiente (termoclina)\n",
    "        # 'campo' deve essereun array 2D (lon, lev)\n",
    "        # 1.0 Calcola il gradiente verticale (derivata rispetto al livello)\n",
    "        DT_Dlev = np.gradient(campo, levels, axis=0) #L'argomento 'levels' specifica la spaziatura (i livelli stessi) per un gradiente non uniforme.\n",
    "        abs_DT_Dlev = np.abs(DT_Dlev)\n",
    "        # 1.1 Trova l'indice del massimo gradiente lungo l'asse 'levels' con argmax\n",
    "        idx_max_grad = np.argmax(abs_DT_Dlev, axis=0) # Risultato: un array 1D (lunghezza 'lon')\n",
    "        # 1.2 Trova la profondita'/livello del massimo gradiente. np.take(a, indices, axis) estrae l'elemento a quell'indice lungo l'asse.\n",
    "        # Vogliamo estrarre il valore di 'lev' [metri] che corrisponde all'indice di 'lon' trovato per ogni 'lon'.\n",
    "        depth_max_grad = levels[idx_max_grad]\n",
    "        # 1.3 Livelli di profondità su cui si registra la termoclina per ogni lon\n",
    "        lon_indices = np.arange(len(longitude))\n",
    "        # 1.4 Usa gli indici (lon, lev) per estrarre i valori dal campo (le temperature)\n",
    "        temp_max_grad = campo[idx_max_grad, lon_indices]\n",
    "        \n",
    "        # 2. Interpolazione, Cubica: usiamo lon come X e depth_max_grad come Y e lo valutiamo (interpolando) sulla longitudine estesa\n",
    "        termoclina_interp = CubicSpline(longitude, depth_max_grad, bc_type='natural')(lon_ext)\n",
    "        for output in [termoclina_interp, depth_max_grad, temp_max_grad]:\n",
    "            if np.any(np.isnan(output)):\n",
    "                logger.warning(\"Risultato contiene NaN\")\n",
    "        return termoclina_interp, depth_max_grad, temp_max_grad\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Tipo di input non supportato. Usa xr.DataArray o np.ndarray.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP\n",
    "# Funzione termoclina OLD non allineata a versione per applicazione per chunks, versione NON dask-friendly.\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask.array as dask_array\n",
    "from scipy.interpolate import interp1d\n",
    "import logging\n",
    "from typing import Tuple, Optional, Union\n",
    "import AllePowerFunctions as apf\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def thermocline(\n",
    "    campo: xr.DataArray,\n",
    "    lon_ext: np.ndarray,\n",
    "    levels: Optional[np.ndarray] = None,\n",
    "    latitude: Optional[np.ndarray] = None,\n",
    "    longitude: Optional[np.ndarray] = None\n",
    ") -> Tuple[xr.DataArray, xr.DataArray, xr.DataArray]:\n",
    "    \"\"\"\n",
    "    Calcola la termoclina come punto di massimo gradiente verticale.\n",
    "    Versione Dask-friendly.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    campo :\n",
    "        1° caso, xr.DataArray: campo di temperatura con dimensioni (time, lev, lat, lon) o subset\n",
    "        2° caso, numpy.ndarray: è importante che le dimensioni siano ordinate come mostrato. Se le dimensioni sono 3 vanno portate a 2, altrimenti è ambiguo.\n",
    "    lon_ext : np.ndarray\n",
    "        Nuove longitudini su cui interpolare la termoclina\n",
    "    levels : np.ndarray, optional\n",
    "        Livelli di profondità (solo per input numpy)\n",
    "    latitude : np.ndarray, optional  \n",
    "        Latitudini (solo per input numpy)\n",
    "    longitude : np.ndarray, optional\n",
    "        Longitudini (solo per input numpy)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    termoclina_interp : xr.DataArray\n",
    "        Profondità della termoclina interpolata su lon_ext\n",
    "    depth_max_grad : xr.DataArray  \n",
    "        Profondità della termoclina per ogni longitudine originale\n",
    "    temp_max_grad : xr.DataArray\n",
    "        Temperatura alla termoclina per ogni longitudine originale\n",
    "    \"\"\"\n",
    "    if isinstance(campo, xr.DataArray):\n",
    "        # 0. Preparazione\n",
    "        ## 0.1 Verifica Chunks\n",
    "        if isinstance(campo.data, dask_array.Array):\n",
    "            lev_chunks = campo.chunks[campo.dims.index('lev')] if 'lev' in campo.dims else ()\n",
    "            if len(lev_chunks) > 1 or (lev_chunks and lev_chunks[0] != campo.lev.size):\n",
    "                raise ValueError(\n",
    "                    \"La dimensione 'lev' deve essere interamente contenuta in un singolo chunk. \"\n",
    "                    \"Usa campo.chunk({'lev': -1}) prima di chiamare questa funzione.\"\n",
    "                )\n",
    "        \n",
    "        ## 0.2 Clona il DataArray per non modificare l'originale\n",
    "        campo_work = campo.copy()\n",
    "        \n",
    "        ## 0.3 Media su 'time' e 'lat'\n",
    "        if 'time' in campo_work.dims:\n",
    "            campo_work = campo_work.mean('time')\n",
    "        if 'lat' in campo_work.dims:\n",
    "            try:\n",
    "                campo_work = apf.meridional_mean(campo_work, latitude=None, mask=None, skip_nan=True, skip_inf=True)\n",
    "            except:\n",
    "                # Fallback a media semplice se meridional_mean non disponibile\n",
    "                campo_work = campo_work.mean('lat')\n",
    "                logger.debug(\"Fallback, media latitudinale non pesata\")\n",
    "        \n",
    "        ## 0.4 Gradiente\n",
    "        dT_dlev = campo_work.differentiate('lev')\n",
    "        abs_dT_dlev = np.abs(dT_dlev)\n",
    "        ## 0.5  Massimo del gradiente (operazione non Dask se il Dataset è chunked)\n",
    "        idx_max_grad = abs_dT_dlev.argmax(dim='lev')\n",
    "        depth_max_grad = campo_work['lev'].isel(lev=idx_max_grad)\n",
    "        temp_max_grad = campo_work.isel(lev=idx_max_grad)\n",
    "        \n",
    "        # 1. Interpolazione\n",
    "        ## 1.1 funzione interpolante per array 1D su lon_ext\n",
    "        def _spline_cubic_1d(x_old, y_old, x_new):\n",
    "            \"\"\"Funzione interna per interpolazione spline 1D\"\"\"\n",
    "            # Gestione valori NaN\n",
    "            valid_mask = np.isfinite(y_old)\n",
    "            if np.sum(valid_mask) < 2:\n",
    "                return np.full_like(x_new, np.nan)\n",
    "            \n",
    "            x_valid = x_old[valid_mask]\n",
    "            y_valid = y_old[valid_mask]\n",
    "            \n",
    "            try:\n",
    "                # Interpolazione spline cubica\n",
    "                interpolator = interp1d(\n",
    "                    x_valid, y_valid, \n",
    "                    kind='cubic', \n",
    "                    bounds_error=False,\n",
    "                    fill_value=np.nan,\n",
    "                    assume_sorted=True\n",
    "                )\n",
    "                return interpolator(x_new)\n",
    "            except (ValueError, TypeError) as e:\n",
    "                # Fallback a interpolazione lineare\n",
    "                logger.debug(f\"Spline failed, using linear: {e}\")\n",
    "                return np.interp(x_new, x_valid, y_valid, left=np.nan, right=np.nan)\n",
    "        \n",
    "        # 1.2 Applica interpolazione usando 'apply_ufunc'\n",
    "        termoclina_interp = xr.apply_ufunc(\n",
    "            _spline_cubic_1d,                                               # funzione da applicare\n",
    "            campo_work['lon'],                                              # ascissa originale: x_old\n",
    "            depth_max_grad,                                                 # dati: y_old\n",
    "            lon_ext,                                                        # nuova ascissa: x_new\n",
    "            input_core_dims=[['lon'], ['lon'], ['lon_ext']],                # specifica le dimensioni da trattare come \"core\" (1D)\n",
    "            output_core_dims=[['lon_ext']],                                 # la dimensione di output\n",
    "            vectorize=True,                                                 # vectorizza su tutte le altre dimensioni (es: lon)\n",
    "            dask='parallelized',                                            # Dask solo per funzioni dask-compatibili\n",
    "            output_dtypes=[depth_max_grad.dtype],                           # data type of the retourned array\n",
    "            dask_gufunc_kwargs={'output_sizes': {'lon_ext': len(lon_ext)}}\n",
    "        )\n",
    "        \n",
    "        # 2. Aggiustamenti e Checks\n",
    "        ## 2.1 Aggiungi coordinate al risultato\n",
    "        termoclina_interp = termoclina_interp.assign_coords({'lon_ext': lon_ext})\n",
    "        termoclina_interp = termoclina_interp.rename({'lon_ext': 'lon'})\n",
    "        \n",
    "        ## 2.2 Controllo (opzionale - da eseguire solo se non lazy)\n",
    "        if not isinstance(termoclina_interp.data, dask_array.Array):\n",
    "            for output in [termoclina_interp, depth_max_grad, temp_max_grad]:\n",
    "                if np.any(np.isnan(output)):\n",
    "                    logger.warning(\"Risultato contiene NaN\")\n",
    "        \n",
    "        return termoclina_interp, depth_max_grad, temp_max_grad\n",
    "    \n",
    "    elif isinstance(campo, xr.Dataset):\n",
    "        raise ValueError(\"Passa un DataArray invece di un Dataset\")\n",
    "    \n",
    "    elif isinstance(campo, np.ndarray):\n",
    "        # 0. Preparazione\n",
    "        ## 0.1 Verifica se le coordinate sono state fornite\n",
    "        if longitude is None:\n",
    "            raise ValueError(\"Se 'campo' è un numpy.ndarray, l'array delle coordinate 'lon' deve essere fornito.\")\n",
    "        \n",
    "        ## 0.2 Media su tempo e latitudine\n",
    "        if np.ndim(campo) == 4:\n",
    "            # Media sull'asse 0 ('time')\n",
    "            campo = np.average(campo, axis=0) \n",
    "            # Media sull'asse 1 (ora asse 0) ('lat')\n",
    "            campo = meridional_mean(campo, latitude=latitude, mask=None, skip_nan=True, skip_inf=True)\n",
    "        elif (np.ndim(campo) == 3) or (np.ndim(campo)>4):\n",
    "            print(\"Ambiguità con 3 dimensioni, riducile a 2!\")\n",
    "        \n",
    "        # 1. Massimo del gradiente (termoclina)\n",
    "        # 'campo' deve essereun array 2D (lon, lev)\n",
    "        # 1.0 Calcola il gradiente verticale (derivata rispetto al livello)\n",
    "        DT_Dlev = np.gradient(campo, levels, axis=0) #L'argomento 'levels' specifica la spaziatura (i livelli stessi) per un gradiente non uniforme.\n",
    "        abs_DT_Dlev = np.abs(DT_Dlev)\n",
    "        # 1.1 Trova l'indice del massimo gradiente lungo l'asse 'levels' con argmax\n",
    "        idx_max_grad = np.argmax(abs_DT_Dlev, axis=0) # Risultato: un array 1D (lunghezza 'lon')\n",
    "        # 1.2 Trova la profondita'/livello del massimo gradiente. np.take(a, indices, axis) estrae l'elemento a quell'indice lungo l'asse.\n",
    "        # Vogliamo estrarre il valore di 'lev' [metri] che corrisponde all'indice di 'lon' trovato per ogni 'lon'.\n",
    "        depth_max_grad = levels[idx_max_grad]\n",
    "        # 1.3 Livelli di profondità su cui si registra la termoclina per ogni lon\n",
    "        lon_indices = np.arange(len(longitude))\n",
    "        # 1.4 Usa gli indici (lon, lev) per estrarre i valori dal campo (le temperature)\n",
    "        temp_max_grad = campo[idx_max_grad, lon_indices]\n",
    "        \n",
    "        # 2. Interpolazione, Cubica: usiamo lon come X e depth_max_grad come Y e lo valutiamo (interpolando) sulla longitudine estesa\n",
    "        termoclina_interp = CubicSpline(longitude, depth_max_grad, bc_type='natural')(lon_ext)\n",
    "        for output in [termoclina_interp, depth_max_grad, temp_max_grad]:\n",
    "            if np.any(np.isnan(output)):\n",
    "                logger.warning(\"Risultato contiene NaN\")\n",
    "        return termoclina_interp, depth_max_grad, temp_max_grad\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Tipo di input non supportato. Usa xr.DataArray o np.ndarray.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d349fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP\n",
    "# Funzione termoclina per applicazione per chunks, versione dask-friendly, DS\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask.array as dask_array\n",
    "from scipy.interpolate import interp1d\n",
    "import logging\n",
    "from typing import Tuple, Optional\n",
    "import AllePowerFunctions as apf\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def thermocline_dask_chunk(\n",
    "    campo: xr.DataArray,\n",
    "    lon_ext: np.ndarray,\n",
    "    chunks: Optional[dict] = None\n",
    ") -> Tuple[xr.DataArray, xr.DataArray, xr.DataArray]:\n",
    "    \"\"\"\n",
    "    Calcola la termoclina come punto di massimo gradiente verticale\n",
    "    Versione Dask-friendly per applicazione per chunks via 'apply_ufunc()' iterando/parallelizzando sulla dimensione tempo\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    campo :\n",
    "        xr.DataArray: campo di temperatura con dimensioni (time, lev, lat, lon) o subset\n",
    "    lon_ext : np.ndarray\n",
    "        Nuove longitudini su cui interpolare la termoclina\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    termoclina_interp : xr.DataArray\n",
    "        Profondità della termoclina interpolata su lon_ext\n",
    "    depth_max_grad : xr.DataArray  \n",
    "        Profondità della termoclina per ogni longitudine originale\n",
    "    temp_max_grad : xr.DataArray\n",
    "        Temperatura alla termoclina per ogni longitudine originale\n",
    "    \"\"\"\n",
    "    if isinstance(campo, xr.DataArray):\n",
    "        # 0. Preparazione\n",
    "        ## 0.1 Verifica Chunks\n",
    "        if isinstance(campo.data, dask_array.Array):\n",
    "            if chunks:\n",
    "                campo = campo.chunk(chunks)\n",
    "                # lev non deve essere chunkata\n",
    "                lev_chunks = chunks[campo.dims.index('lev')] if 'lev' in campo.dims else ()\n",
    "                if len(lev_chunks) > 1 or (lev_chunks and lev_chunks[0] != campo.lev.size):\n",
    "                    raise ValueError(\n",
    "                        \"La dimensione 'lev' deve essere interamente contenuta in un singolo chunk. \"\n",
    "                        \"Usa campo.chunk({'lev': -1}) prima di chiamare questa funzione.\"\n",
    "                    )                \n",
    "            else:\n",
    "                # Chunks di default\n",
    "                campo = campo.chunk({'time': 120, 'lev': -1, 'lon': 27})            \n",
    "        \n",
    "        ## 0.2 Clona il DataArray per non modificare l'originale\n",
    "        campo_work = campo.copy()\n",
    "        \n",
    "        ## 0.3 Media su 'lat'\n",
    "        if 'lat' in campo_work.dims:\n",
    "            try:\n",
    "                campo_work = apf.meridional_mean(campo_work, latitude=None, mask=None, skip_nan=True, skip_inf=True)\n",
    "            except:\n",
    "                # Fallback a media semplice se meridional_mean non disponibile\n",
    "                campo_work = campo_work.mean('lat')\n",
    "                logger.debug(\"Fallback, media latitudinale non pesata\")\n",
    "\n",
    "        ## 0.4 Gradiente\n",
    "        dT_dlev = campo_work.differentiate('lev')\n",
    "        abs_dT_dlev = np.abs(dT_dlev)\n",
    "        \n",
    "        ## 0.5 Massimo del gradiente usando funzione apposita\n",
    "        #il fancy indexing con 'isel' non è gestito da Dask, quindi non posso fare semplicemente come segue,\n",
    "        #devo passare da numpy e di conseguenza apply_ufunc\n",
    "        #idx_max_grad = abs_dT_dlev.argmax(dim='lev')\n",
    "        #depth_max_grad = campo_work['lev'].isel(lev=idx_max_grad)\n",
    "        #temp_max_grad = campo_work.isel(lev=idx_max_grad)\n",
    "        def _find_max_grad(temperature, levels, abs_grad):\n",
    "            \"\"\"\n",
    "            Trova indice, profondità e temperatura del massimo gradiente.\n",
    "            \"\"\"\n",
    "            # Trova indice del massimo gradiente\n",
    "            idx_max = np.nanargmax(abs_grad, axis=0)\n",
    "            \n",
    "            # Estrai profondità corrispondente\n",
    "            depth_max = np.take_along_axis(\n",
    "                levels.reshape(-1, 1), \n",
    "                idx_max.reshape(1, -1), \n",
    "                axis=0\n",
    "            ).squeeze()\n",
    "            \n",
    "            # Estrai temperatura corrispondente\n",
    "            temp_max = np.take_along_axis(\n",
    "                temperature, \n",
    "                idx_max.reshape(1, -1), \n",
    "                axis=0\n",
    "            ).squeeze()\n",
    "            \n",
    "            return depth_max, temp_max, idx_max\n",
    "        \n",
    "        ### 0.5.1 Applica la funzione usando 'apply_ufunc'\n",
    "        depth_max_grad, temp_max_grad, idx_max_grad = xr.apply_ufunc(\n",
    "            _find_max_grad,\n",
    "            campo_work,\n",
    "            campo_work['lev'],\n",
    "            abs_dT_dlev,\n",
    "            input_core_dims=[['lev', 'lon'], ['lev'], ['lev', 'lon']],\n",
    "            output_core_dims=[['lon'], ['lon'], ['lon']],\n",
    "            vectorize=True,\n",
    "            dask='parallelized',\n",
    "            output_dtypes=[campo_work['lev'].dtype, campo_work.dtype, np.int64]\n",
    "        )\n",
    "        \n",
    "        # 1. Interpolazione\n",
    "        ## 1.1 funzione interpolante per array 1D su lon_ext\n",
    "        def _spline_cubic_1d(x_old, y_old, x_new):\n",
    "            \"\"\"Funzione interna per interpolazione spline 1D\"\"\"\n",
    "            # Gestione valori NaN\n",
    "            valid_mask = np.isfinite(y_old)\n",
    "            if np.sum(valid_mask) < 2:\n",
    "                return np.full_like(x_new, np.nan)\n",
    "            \n",
    "            x_valid = x_old[valid_mask]\n",
    "            y_valid = y_old[valid_mask]\n",
    "            \n",
    "            try:\n",
    "                # Interpolazione spline cubica\n",
    "                interpolator = interp1d(\n",
    "                    x_valid, y_valid, \n",
    "                    kind='cubic', \n",
    "                    bounds_error=False,\n",
    "                    fill_value=np.nan,\n",
    "                    assume_sorted=True\n",
    "                )\n",
    "                return interpolator(x_new)\n",
    "            except (ValueError, TypeError) as e:\n",
    "                # Fallback a interpolazione lineare\n",
    "                logger.debug(f\"Spline failed, using linear: {e}\")\n",
    "                return np.interp(x_new, x_valid, y_valid, left=np.nan, right=np.nan)\n",
    "        \n",
    "        ## 1.2 Applica interpolazione usando 'apply_ufunc'\n",
    "        termoclina_interp = xr.apply_ufunc(\n",
    "            _spline_cubic_1d,                                               # funzione da applicare\n",
    "            campo_work['lon'],                                              # ascissa originale: x_old\n",
    "            depth_max_grad,                                                 # dati: y_old\n",
    "            lon_ext,                                                        # nuova ascissa: x_new\n",
    "            input_core_dims=[['lon'], ['lon'], ['lon_ext']],                # specifica le dimensioni da trattare come \"core\" (1D)\n",
    "            output_core_dims=[['lon_ext']],                                 # la dimensione di output\n",
    "            vectorize=True,                                                 # vectorizza su tutte le altre dimensioni (es: lon)\n",
    "            dask='parallelized',                                            # Dask solo per funzioni dask-compatibili\n",
    "            output_dtypes=[depth_max_grad.dtype],                           # data type of the retourned array\n",
    "            dask_gufunc_kwargs={'output_sizes': {'lon_ext': len(lon_ext)}}\n",
    "        )\n",
    "        \n",
    "        # 2. Aggiustamenti e Checks\n",
    "        ## 2.1 Aggiungi coordinate al risultato\n",
    "        termoclina_interp = termoclina_interp.assign_coords({'lon_ext': lon_ext})\n",
    "        termoclina_interp = termoclina_interp.rename({'lon_ext': 'lon'})\n",
    "        \n",
    "        ## 2.2 Controllo (opzionale - da eseguire solo se non lazy)\n",
    "        if not isinstance(termoclina_interp.data, dask_array.Array):\n",
    "            for output in [termoclina_interp, depth_max_grad, temp_max_grad]:\n",
    "                if np.any(np.isnan(output)):\n",
    "                    logger.warning(\"Risultato contiene NaN\")\n",
    "        \n",
    "        return termoclina_interp, depth_max_grad, temp_max_grad\n",
    "    else:\n",
    "        raise TypeError(\"Tipo di input non supportato. Usa xr.DataArray o np.ndarray.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a50887",
   "metadata": {},
   "outputs": [],
   "source": [
    "----------------------------------------------------------------\n",
    "# Funzione complessa non testata di DS che dovrebbe evitare la fancy indexing, ma ho già risolto questo problema. non la userei/considererei ulteriormente. da eliminare al più presto in caso \n",
    "def thermocline_dask_large_datasets(\n",
    "    campo: xr.DataArray,\n",
    "    lon_ext: np.ndarray,\n",
    "    chunks: Optional[dict] = None\n",
    ") -> Tuple[xr.DataArray, xr.DataArray, xr.DataArray]:\n",
    "    \"\"\"\n",
    "    Versione ottimizzata per dataset LARGE con chunk temporali.\n",
    "    \"\"\"\n",
    "    # 1. ASSICURA I CHUNKS CORRETTI\n",
    "    if chunks:\n",
    "        campo = campo.chunk(chunks)\n",
    "    else:\n",
    "        # Chunks di default per il tuo caso\n",
    "        campo = campo.chunk({'time': 120, 'lev': -1, 'lon': 27})\n",
    "    \n",
    "    # 2. MEDIA SU LAT se presente (lazy)\n",
    "    if 'lat' in campo.dims:\n",
    "        campo = campo.mean('lat')\n",
    "    \n",
    "    # 3. FUNZIONE per processare CHUNK TEMPORALI\n",
    "    def _process_time_chunk(temp_chunk, lev, lon):\n",
    "        \"\"\"\n",
    "        Processa un chunk temporale (time, lev, lon) -> (time, lon)\n",
    "        \"\"\"\n",
    "        n_time, n_lev, n_lon = temp_chunk.shape\n",
    "        depth_results = np.empty((n_time, n_lon), dtype=temp_chunk.dtype)\n",
    "        temp_results = np.empty((n_time, n_lon), dtype=temp_chunk.dtype)\n",
    "        \n",
    "        for t in range(n_time):\n",
    "            for i in range(n_lon):\n",
    "                profile = temp_chunk[t, :, i]\n",
    "                \n",
    "                # Gradiente (differenza finita)\n",
    "                grad = np.abs(np.gradient(profile, lev))\n",
    "                \n",
    "                if np.all(np.isnan(grad)):\n",
    "                    depth_results[t, i] = np.nan\n",
    "                    temp_results[t, i] = np.nan\n",
    "                else:\n",
    "                    idx_max = np.nanargmax(grad)\n",
    "                    depth_results[t, i] = lev[idx_max]\n",
    "                    temp_results[t, i] = profile[idx_max]\n",
    "        \n",
    "        return depth_results, temp_results\n",
    "    \n",
    "    # 4. APPLICA SU OGNI CHUNK TEMPORALE (lazy!)\n",
    "    lev_vals = campo['lev'].values\n",
    "    lon_vals = campo['lon'].values\n",
    "    \n",
    "    if isinstance(campo.data, dask_array.Array):\n",
    "        # Usa map_blocks per processare chunk temporali\n",
    "        depth_temp_data = dask_array.map_blocks(\n",
    "            lambda x: _process_time_chunk(x, lev_vals, lon_vals),\n",
    "            campo.data,\n",
    "            dtype=campo.dtype,\n",
    "            chunks=(campo.chunks[0], campo.chunks[2]),  # (time_chunks, lon)\n",
    "            new_axis=None\n",
    "        )\n",
    "        \n",
    "        # depth_temp_data è una tupla\n",
    "        depth_max_grad = xr.DataArray(\n",
    "            depth_temp_data[0],\n",
    "            dims=['time', 'lon'],\n",
    "            coords={'time': campo.time, 'lon': campo.lon}\n",
    "        )\n",
    "        temp_max_grad = xr.DataArray(\n",
    "            depth_temp_data[1],\n",
    "            dims=['time', 'lon'],\n",
    "            coords={'time': campo.time, 'lon': campo.lon}\n",
    "        )\n",
    "    else:\n",
    "        # Versione numpy (non dovresti usarla per dataset così grandi)\n",
    "        depth_arr, temp_arr = _process_time_chunk(campo.data, lev_vals, lon_vals)\n",
    "        depth_max_grad = xr.DataArray(depth_arr, dims=['time', 'lon'], \n",
    "                                     coords={'time': campo.time, 'lon': campo.lon})\n",
    "        temp_max_grad = xr.DataArray(temp_arr, dims=['time', 'lon'],\n",
    "                                    coords={'time': campo.time, 'lon': campo.lon})\n",
    "    \n",
    "    # 5. INTERPOLAZIONE PER OGNI TIME STEP (lazy!)\n",
    "    def _interpolate_per_time(x_old, y_old, x_new):\n",
    "        \"\"\"Interpolazione per ogni time step separatamente.\"\"\"\n",
    "        n_time = y_old.shape[0] if y_old.ndim > 1 else 1\n",
    "        n_new = len(x_new)\n",
    "        \n",
    "        if y_old.ndim == 1:\n",
    "            y_old = y_old.reshape(1, -1)\n",
    "        \n",
    "        result = np.empty((n_time, n_new), dtype=y_old.dtype)\n",
    "        \n",
    "        for t in range(n_time):\n",
    "            valid_mask = np.isfinite(y_old[t, :])\n",
    "            if np.sum(valid_mask) < 2:\n",
    "                result[t, :] = np.nan\n",
    "            else:\n",
    "                result[t, :] = np.interp(\n",
    "                    x_new, \n",
    "                    x_old[valid_mask], \n",
    "                    y_old[t, valid_mask],\n",
    "                    left=np.nan, right=np.nan\n",
    "                )\n",
    "        \n",
    "        return result.squeeze()\n",
    "    \n",
    "    # Applica interpolazione mantenendo chunks temporali\n",
    "    termoclina_interp = xr.apply_ufunc(\n",
    "        _interpolate_per_time,\n",
    "        depth_max_grad['lon'],\n",
    "        depth_max_grad,\n",
    "        lon_ext,\n",
    "        input_core_dims=[['lon'], ['time', 'lon'], ['lon_ext']],\n",
    "        output_core_dims=[['time', 'lon_ext']],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[depth_max_grad.dtype],\n",
    "        dask_gufunc_kwargs={'output_sizes': {'lon_ext': len(lon_ext)}}\n",
    "    )\n",
    "    \n",
    "    # Aggiungi coordinate\n",
    "    termoclina_interp = termoclina_interp.assign_coords({\n",
    "        'time': campo.time,\n",
    "        'lon_ext': lon_ext\n",
    "    }).rename({'lon_ext': 'lon'})\n",
    "    \n",
    "    return termoclina_interp, depth_max_grad, temp_max_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d3454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf17ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f2c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c287e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
